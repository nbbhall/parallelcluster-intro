# High Performance Computing (HPC) - Intro

HPC is the ability to perform **parallel** data processing to **improve computing performance** and **perform complex calculations** at high speeds

It's popular applications include AI/ML, trend analysis, test simulation, etc

## HPC typically has
- A cluster of co-located multi-node compute (CPU, GPU, RAM)
- High network bandwidth
- Shared storage
- job orchestration engine (scheduler)

<p>&nbsp;</p>

## HPC on Cloud v/s HPC on-premise

- on-demand, virtually infinite compute capacity
- Elasticity – scale up and down as required; pay only for what you use
- Choice of infrastructure (Spot, On-demand, Graviton, C5n)

<p>&nbsp;</p>

## Why ParallelCluster?

- Provides ease of HPC cluster provisioning and management (through text based configuration files)
- Provides automatic compute resource scaling (on-demand or spot)
- Automatic shared storage scaling (using EFS or FSx)
- Ease of migration from on-premise (slurm job scheduler)
- Ease of integration with other AWS services
- Open source

<p>&nbsp;</p>

## Architecture

- client (pcluster)
- head node
- compute node
- job scheduler
- EFS / FSx for Lusture (shared FS)

![](pc-arch.png)

<p>&nbsp;</p>

## User Workflow 

![](workflow.png)

<p>&nbsp;</p>

## Main Components

- job scheduler (slurm or awsbatch)
- head node
- compute nodes
- EFA
- EFS / FSx for Lusture (shared file system between master and compute nodes)

<p>&nbsp;</p>

## Other Components - clustermgtd (Slurm only)

- Syncs scheduler with EC2 status
- Static capacity management (ensure required number of EC2 are up)
- Unhealthy Amazon EC2 instances management
- Unhealthy Scheduler nodes management

<p>&nbsp;</p>

## Other Components - computemgtd (Slurm only)

- compute management daemon running on each compute node
- Syncs with head node every 5 mins
- Shuts down if it cannot reach head node

<p>&nbsp;</p>

## Other Components – nodewatcher (both slurm and awabatch)

- Runs on each node in the compute fleet
- monitors node and scales down if 
    - no job on instance for more than 10 mins (scaledown_idletime)
AND
    - no pending jobs 

![](nodewatcher.png)

<p>&nbsp;</p>

## Network Configuration

### VPC requirements
- DNS Resolution = yes
- DNS Hostnames = yes

<p>&nbsp;</p>

### Supported configurations
- One subnet for both head and compute nodes
- Two subnets, with the head node in one public subnet, and compute nodes in a private subnet (with NAT gateway)

[supported network configuration](https://docs.aws.amazon.com/parallelcluster/latest/ug/network-configuration-v3.html)

<p>&nbsp;</p>

## Job Schedulers

[Slurm](https://slurm.schedmd.com)

[awsbatch](https://aws.amazon.com/batch)

<p>&nbsp;</p>

## ParallelCluster Install - requirements

- Linux/MacOS
- Python
- AWS CLI
- pip
- virtualenv (optional)

<p>&nbsp;</p>

## ParallelCluster Install - Lab

[Supported regions](https://docs.aws.amazon.com/parallelcluster/latest/ug/supported-regions.html)

<p>&nbsp;</p>

### High level steps
- Create a workstation (CloudFormation)
- Setup pre-req on workstation
- Install pcluster

<p>&nbsp;</p>

## Out of scope
- code troubleshooting
- code support
- only infrastructure is supported


<p>&nbsp;</p>

## Installation of **pcluster** CLI

```
aws configure # set region only


sudo yum install python37 -y
curl -O https://bootstrap.pypa.io/get-pip.py
python3 get-pip.py
python3 -m pip install --upgrade pip
python3 -m pip install --user --upgrade virtualenv
python3 -m virtualenv ~/apc-ve


source ~/apc-ve/bin/activate      # needs to be performed in every new session

python3 -m pip install --upgrade "aws-parallelcluster"
(apc-ve) [root@ip-172-31-4-74 ~]# pcluster version
{
"version": "3.3.1"
}
# Node.js installation – required by CDKcurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash
chmod ug+x ~/.nvm/nvm.sh
source ~/.nvm/nvm.sh # needs to be performed in every new session
# nvm install --lts
nvm install --lts=Gallium # fixes "Unable to check Node.js version" error
node --version
```

<p>&nbsp;</p>

## pcluster CLI – sub-commands

```
list-clusters       	    	Retrieve the list of existing clusters.
create-cluster      	    	Create a managed cluster in a given region.
delete-cluster      	    	Initiate the deletion of a cluster.
describe-cluster    	    	Get detailed information about an existing cluster.
update-cluster      	    	Update a cluster managed in a given region.
describe-compute-fleet        	Describe the status of the compute fleet.
update-compute-fleet          	Update the status of the cluster compute fleet.
delete-cluster-instances      	Initiate the forced termination of all cluster 			compute nodes. Does not work with AWS Batch 			clusters.
describe-cluster-instances    	Describe the instances belonging to a given 			cluster.
list-cluster-log-streams      	Retrieve the list of log streams associated with a 			cluster.
get-cluster-log-events	Retrieve the events associated with a log stream.
get-cluster-stack-events	Retrieve the events associated with the stack for a 			given cluster.
list-images         		Retrieve the list of existing custom images.
build-image         		Create a custom ParallelCluster image in a given 			region.
delete-image        		Initiate the deletion of the custom ParallelCluster 			image.
describe-image      		Get detailed information about an existing image.
list-image-log-streams 	Retrieve the list of log streams associated with an 			image.
get-image-log-events  	Retrieve the events associated with an image build.
get-image-stack-events    	Retrieve the events associated with the stack for a 			given image build.
list-official-images 		List Official ParallelCluster AMIs.
configure           		Start the AWS ParallelCluster configuration.
dcv-connect         		Permits to connect to the head node through an 			interactive session by using NICE DCV.
export-cluster-logs 		Export the logs of the cluster to a local tar.gz 			archive by passing through an Amazon S3 Bucket.
export-image-logs   		Export the logs of the image builder stack to a 			local tar.gz archive by passing through an Amazon 			S3 Bucket.
ssh                 		Connects to the head node instance using SSH.
version           	  	Displays the version of AWS ParallelCluster.

```

<p>&nbsp;</p>

## pcluster configure (awsbatch) - Lab

```
pcluster configure --config cluster-config-awsbatch.yaml
AWS Region ID [us-east-1]: 
EC2 Key Pair Name: [Set desired]
Scheduler [slurm]: 2
Head node instance type [t2.micro]: t2.medium
Name of queue 1 [queue1]: 
Maximum vCPU [10]: 
Automate VPC creation? (y/n) [n]: n
Allowed values for VPC ID: [Choose desired]
Automate Subnet creation? (y/n) [y]: n
head node subnet ID: [Choose desired]
compute subnet ID: [Choose desired]

Configuration file written to cluster-config-awsbatch.yaml
You can edit your configuration file or simply run 'pcluster create-cluster --cluster-configuration cluster-config-awsbatch.yaml --cluster-name cluster-name --region us-east-1' to create your cluster.

Copy SSH key to jump box and chmod 400

```

<p>&nbsp;</p>

## pcluster create-cluster (awsbatch) - Lab

```
$ pcluster create-cluster --cluster-configuration cluster-config-awsbatch.yaml --cluster-name cluster-name --region us-east-1

{
  "cluster": {
    "clusterName": "cluster-name",
    "cloudformationStackStatus": "CREATE_IN_PROGRESS",
    "cloudformationStackArn": "arn:aws:cloudformation:us-east-1:419264005465:stack/cluster-name/ca529720-79c2-11ed-9fb1-0e23c4409c71",
    "region": "us-east-1",
    "version": "3.3.1",
    "clusterStatus": "CREATE_IN_PROGRESS",
    "scheduler": {
      "type": "awsbatch"
    }
  }
}

$ pcluster list-clusters


$ pcluster describe-cluster -n cluster-name
```

<p>&nbsp;</p>

## What happened behind the scenes?

### pcluster created several resources through CloudFormation, some of which are
- HeadNode
- ECR repo
- code build project (which built a Docker image and pushed it to ECR repo)
- managed Batch CE
- job queue
- ECS cluster
- a standard job def
- CW log group

### Inspect below in console
- Batch CE
- Job Queue
- Job Definition 
- CW log group
